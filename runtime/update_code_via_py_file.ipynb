{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69821b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import doctest\n",
    "import datetime\n",
    "import importlib\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "from types import ModuleType\n",
    "from typing import Optional, List, Dict, Union\n",
    "from enum import Enum\n",
    "from uuid import UUID, uuid4\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from pydantic import BaseModel, Field, ConstrainedStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da049853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TREncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, UUID):\n",
    "            return str(obj)\n",
    "        if isinstance(obj, datetime.datetime):\n",
    "            return pd.Timestamp(obj).isoformat()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f53671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataType(str, Enum):\n",
    "    \"\"\"hetida designer data types\n",
    "\n",
    "    These are the types available for component/workflow inputs/outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    Integer = \"INT\"\n",
    "    Float = \"FLOAT\"\n",
    "    String = \"STRING\"\n",
    "    DataFrame = \"DATAFRAME\"\n",
    "    Series = \"SERIES\"\n",
    "    Boolean = \"BOOLEAN\"\n",
    "    Any = \"ANY\"\n",
    "    PlotlyJson = \"PLOTLYJSON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc072ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow only some special characters for category, description, name and version tag\n",
    "ALLOWED_CHARS_RAW_STRING = (\n",
    "    r\"\\w ,\\.\\-\\(\\)=/\"  # pylint: disable=anomalous-backslash-in-string\n",
    ")\n",
    "# The special sequence \\w matches unicode word characters;\n",
    "# this includes most characters that can be part of a word in any language, as well as numbers\n",
    "# and the underscore. If the ASCII flag is used, only [a-zA-Z0-9_] is matched.\n",
    "\n",
    "class NonEmptyValidStr(ConstrainedStr):\n",
    "    min_length = 1\n",
    "    max_length = 60\n",
    "    regex = re.compile(rf\"^[{ALLOWED_CHARS_RAW_STRING}]+$\")\n",
    "\n",
    "\n",
    "class ShortNonEmptyValidStr(ConstrainedStr):\n",
    "    min_length = 1\n",
    "    max_length = 20\n",
    "    regex = re.compile(rf\"^[{ALLOWED_CHARS_RAW_STRING}]+$\")\n",
    "\n",
    "\n",
    "class ValidStr(ConstrainedStr):\n",
    "    regex = re.compile(rf\"^[{ALLOWED_CHARS_RAW_STRING}]*$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e92a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(str, Enum):\n",
    "    \"\"\"Representing state of component/workflow\"\"\"\n",
    "\n",
    "    DRAFT = \"DRAFT\"\n",
    "    RELEASED = \"RELEASED\"\n",
    "    DISABLED = \"DISABLED\"\n",
    "\n",
    "\n",
    "class Type(str, Enum):\n",
    "    COMPONENT = \"COMPONENT\"\n",
    "    WORKFLOW = \"WORKFLOW\"\n",
    "    \n",
    "\n",
    "class IO(BaseModel):\n",
    "    id: UUID = Field(default_factory=uuid4)\n",
    "    name: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Must be a valid python identifier because it will be used for computation\",\n",
    "    )\n",
    "    data_type: DataType\n",
    "\n",
    "\n",
    "class IOInterface(BaseModel):\n",
    "    \"\"\"Represents combination of inputs and outputs.\n",
    "\n",
    "    Note: The names in the list of inputs and outputs must be unique, respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs: List[IO] = []\n",
    "    outputs: List[IO] = []\n",
    "        \n",
    "\n",
    "class RefIdType(str, Enum):\n",
    "    \"\"\"Reference Id type as required for some adapters (notably generic rest adapter)\"\"\"\n",
    "\n",
    "    SOURCE = \"SOURCE\"\n",
    "    SINK = \"SINK\"\n",
    "    THINGNODE = \"THINGNODE\"\n",
    "\n",
    "    \n",
    "class ExternalType(str, Enum):\n",
    "    METADATA_INT = \"metadata(int)\"\n",
    "    METADATA_FLOAT = \"metadata(float)\"\n",
    "    METADATA_STR = \"metadata(str)\"\n",
    "    METADATA_BOOLEAN = \"metadata(bool)\"\n",
    "    METADATA_ANY = \"metadata(any)\"\n",
    "\n",
    "    TIMESERIES_INT = \"timeseries(int)\"\n",
    "    TIMESERIES_FLOAT = \"timeseries(float)\"\n",
    "    TIMESERIES_STR = \"timeseries(str)\"\n",
    "    TIMESERIES_BOOLEAN = \"timeseries(bool)\"\n",
    "    TIMESERIES_ANY = \"timeseries(any)\"\n",
    "\n",
    "    SERIES_INT = \"series(int)\"\n",
    "    SERIES_FLOAT = \"series(float)\"\n",
    "    SERIES_STR = \"series(str)\"\n",
    "    SERIES_BOOLEAN = \"series(bool)\"\n",
    "    SERIES_ANY = \"series(any)\"\n",
    "\n",
    "    DATAFRAME = \"dataframe\"\n",
    "    \n",
    "\n",
    "class InputWiring(BaseModel):\n",
    "    workflow_input_name: str = Field(..., alias=\"workflow_input_name\")\n",
    "    adapter_id: Union[int, str] = Field(..., alias=\"adapter_id\")\n",
    "\n",
    "    ref_id: Optional[str] = Field(\n",
    "        None,\n",
    "        description=(\n",
    "            \"Id referencing the source in external systems.\"\n",
    "            \" Not necessary for direct provisioning.\"\n",
    "        ),\n",
    "    )\n",
    "    ref_id_type: Optional[RefIdType] = Field(\n",
    "        None,\n",
    "        description=\"Required if type is specified and is a metadata type. \"\n",
    "        \"Then describes to what kind of object in the tree the metadatum is attached. \"\n",
    "        \"Must then be one of \"\n",
    "        \", \".join(['\"' + x.value + '\"' for x in list(RefIdType)]),\n",
    "    )\n",
    "    ref_key: Optional[str] = None\n",
    "    type: Optional[ExternalType] = Field(\n",
    "        None,\n",
    "        description=\"Type of data. If present then must be one of \"\n",
    "        + \", \".join(['\"' + x.value + '\"' for x in list(ExternalType)]),\n",
    "    )\n",
    "    filters: dict = {}\n",
    "        \n",
    "        \n",
    "class OutputWiring(BaseModel):\n",
    "    workflow_output_name: str = Field(..., alias=\"workflow_output_name\")\n",
    "    adapter_id: Union[int, str] = Field(..., alias=\"adapter_id\")\n",
    "    ref_id: Optional[str] = Field(\n",
    "        None,\n",
    "        description=(\n",
    "            \"Id referencing the sink in external systems.\"\n",
    "            \" Not necessary for direct provisioning.\"\n",
    "        ),\n",
    "    )\n",
    "    ref_id_type: Optional[RefIdType] = Field(\n",
    "        None,\n",
    "        description=\"Required if type is specified and is a metadata type. \"\n",
    "        \"Then describes to what kind of object in the tree the metadatum is attached. \"\n",
    "        \"Must then be one of \"\n",
    "        \", \".join(['\"' + x.value + '\"' for x in list(RefIdType)]),\n",
    "    )\n",
    "    ref_key: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Required if type is specified and is a metadata type. \"\n",
    "        \"Then is the key of the metdatum.\",\n",
    "    )\n",
    "    type: Optional[ExternalType] = Field(\n",
    "        None,\n",
    "        description=\"Type of data. If present then must be one of \"\n",
    "        + \", \".join(['\"' + x.value + '\"' for x in list(ExternalType)]),\n",
    "    )\n",
    "        \n",
    "        \n",
    "class WorkflowWiring(BaseModel):\n",
    "    input_wirings: List[InputWiring] = []\n",
    "    output_wirings: List[OutputWiring] = []\n",
    "        \n",
    "\n",
    "class TransformationRevision(BaseModel):\n",
    "    \"\"\"Either a component revision or a workflow revision\n",
    "\n",
    "    Both can be instantiated as an operator in a workflow revision\n",
    "    (yes, workflow in workflow in workflow... is possible) and are therefore\n",
    "    able to transform input data to output result data.\n",
    "\n",
    "    Note that there is no actual component or workflow entity, only revisions. Revisions are tied\n",
    "    together via the group id, and otherwise do not need to have anything in common, i.e. their\n",
    "    name and their interface etc. can differ completely.\n",
    "\n",
    "    Revisions with state RELEASED are what makes execution reproducible - they cannot be edited any\n",
    "    more and only they can be instantiated as operators.\n",
    "\n",
    "    Additionally RELEASED revisions cannot be deleted, but their state can be changed to\n",
    "    DISABLED. DISABLED revisions cannot be instantiated as new operators anymore but existing\n",
    "    operators from them still work (for reproducibility). Note that in the Frontend the DISABLED\n",
    "    state is called \"DEPRECATED\". The frontend then allows to replace deprecated operators by other\n",
    "    (possibly newer) released revisions from the the same revision group (i.e. same group id).\n",
    "    \"\"\"\n",
    "\n",
    "    id: UUID\n",
    "    revision_group_id: UUID\n",
    "    name: str\n",
    "    description: str = \"\"\n",
    "    category: str = Field(\n",
    "        \"Other\",\n",
    "        description='Category in which this is classified, i.e. the \"drawer\" in the User Interface',\n",
    "    )\n",
    "    version_tag: str\n",
    "    released_timestamp: Optional[datetime.datetime] = Field(\n",
    "        None,\n",
    "        description=\"If the revision is RELEASED then this should be release timestamp\",\n",
    "    )\n",
    "\n",
    "    disabled_timestamp: Optional[datetime.datetime] = Field(\n",
    "        None,\n",
    "        description=\"If the revision is DISABLED then this should be disable/deprecation timestamp\",\n",
    "    )\n",
    "    state: State = Field(\n",
    "        ...,\n",
    "        description=\"one of \" + \", \".join(['\"' + x.value + '\"' for x in list(State)]),\n",
    "    )\n",
    "    type: Type = Field(\n",
    "        ...,\n",
    "        description=\"one of \" + \", \".join(['\"' + x.value + '\"' for x in list(Type)]),\n",
    "    )\n",
    "\n",
    "    documentation: str = Field(\n",
    "        (\n",
    "            \"\\n\"\n",
    "            \"# New Component/Workflow\\n\"\n",
    "            \"## Description\\n\"\n",
    "            \"## Inputs\\n\"\n",
    "            \"## Outputs\\n\"\n",
    "            \"## Details\\n\"\n",
    "            \"## Examples\\n\"\n",
    "        ),\n",
    "        description=\"Documentation in markdown format.\",\n",
    "    )\n",
    "    content: Union[str,dict]\n",
    "\n",
    "    io_interface: IOInterface = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"In case of type WORKFLOW determined from content. \"\n",
    "            \"To change from state DRAFT to state RELEASED all inputs and outputs must have names.\"\n",
    "        ),\n",
    "    )\n",
    "        \n",
    "    test_wiring: WorkflowWiring = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"The input and output wirings must match \"\n",
    "            \"the inputs and outputs of the io_interface\"\n",
    "        ),\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e346f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComponentInfo(BaseModel):\n",
    "    \"\"\"Provide meta-information about component.\n",
    "\n",
    "    Used as input for code generation to include meta-information about the component in the code.\n",
    "\n",
    "    This additional information makes it possible to recover the underlying transformation revision\n",
    "    object from the code.\n",
    "    \"\"\"\n",
    "\n",
    "    input_types_by_name: Dict[str, DataType]\n",
    "    output_types_by_name: Dict[str, DataType]\n",
    "    id: UUID = Field(default_factory=uuid4)\n",
    "    revision_group_id: UUID = Field(default_factory=uuid4)\n",
    "    name: NonEmptyValidStr\n",
    "    category: NonEmptyValidStr\n",
    "    description: ValidStr\n",
    "    version_tag: ShortNonEmptyValidStr\n",
    "    is_coroutine: bool = False\n",
    "        \n",
    "    @classmethod\n",
    "    def from_tr(cls, tr: TransformationRevision) -> \"ComponentInfo\":\n",
    "        return ComponentInfo(\n",
    "            input_types_by_name={io.name: io.data_type for io in tr.io_interface.inputs},\n",
    "            output_types_by_name={io.name: io.data_type for io in tr.io_interface.outputs},\n",
    "            id=tr.id,\n",
    "            revision_group_id=tr.revision_group_id,\n",
    "            name=tr.name,\n",
    "            category=tr.category,\n",
    "            description=tr.description,\n",
    "            version_tag=tr.version_tag,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_template: str = \"\"\"\\\n",
    "# add your imports here, e.g.\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "function_definition_template: str = \"\"\"\\\n",
    "# ***** DO NOT EDIT LINES BELOW *****\n",
    "# These lines may be overwritten if component details or inputs/outputs change.\n",
    "COMPONENT_INFO = {opening_bracket}\n",
    "    \"inputs\": {input_dict_content},\n",
    "    \"outputs\": {output_dict_content},\n",
    "    \"name\": {name},\n",
    "    \"description\": {description},\n",
    "    \"category\": {category},\n",
    "    \"id\": {id},\n",
    "    \"revision_group_id\": {revision_group_id},\n",
    "    \"version_tag\": {version_tag},\n",
    "{closing_bracket}\n",
    "\n",
    "\n",
    "{main_func_declaration_start} main({params_list}):\n",
    "    # entrypoint function for this component\n",
    "    # ***** DO NOT EDIT LINES ABOVE *****\\\n",
    "\"\"\"\n",
    "\n",
    "function_body_template: str = \"\"\"\\\n",
    "    # write your function code here.\n",
    "    pass\\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_function_header(component_info: ComponentInfo) -> str:\n",
    "    \"\"\"Generate entrypoint function header from the inputs and their types\"\"\"\n",
    "    param_list_str = (\n",
    "        \"\"\n",
    "        if len(component_info.input_types_by_name.keys()) == 0\n",
    "        else \"*, \" + \", \".join(component_info.input_types_by_name.keys())\n",
    "    )\n",
    "\n",
    "    main_func_declaration_start = \"async def\" if component_info.is_coroutine else \"def\"\n",
    "    \n",
    "    input_dict_content=\"{\" + \", \".join(\n",
    "        [\n",
    "            '\"' + parameter + '\": \"' + data_type_enum.value + '\"'\n",
    "            for parameter, data_type_enum in component_info.input_types_by_name.items()\n",
    "        ]\n",
    "    ) + \"}\"\n",
    "    # black prefers entries per line for more than 88 characters, 15 are already taken\n",
    "    if len(input_dict_content) > 73:\n",
    "        input_dict_content=\"{\\n        \" + \",\\n        \".join(\n",
    "            [\n",
    "                '\"' + parameter + '\": \"' + data_type_enum.value + '\"'\n",
    "                for parameter, data_type_enum in component_info.input_types_by_name.items()\n",
    "            ]\n",
    "        ) + \",\\n    }\"\n",
    "        \n",
    "    output_dict_content=\"{\" + \", \".join(\n",
    "        [\n",
    "            '\"' + parameter + '\": \"' + data_type_enum.value + '\"'\n",
    "            for parameter, data_type_enum in component_info.output_types_by_name.items()\n",
    "        ]\n",
    "    ) + \"}\"\n",
    "    # black prefers entries per line for more than 88 characters, 16 are already taken\n",
    "    if len(output_dict_content) > 72:\n",
    "        output_dict_content=\"{\\n        \" + \",\\n        \".join(\n",
    "            [\n",
    "                '\"' + parameter + '\": \"' + data_type_enum.value + '\"'\n",
    "                for parameter, data_type_enum in component_info.output_types_by_name.items()\n",
    "            ]\n",
    "        ) + \",\\n    }\"\n",
    "\n",
    "    return function_definition_template.format(\n",
    "        input_dict_content=input_dict_content,\n",
    "        output_dict_content=output_dict_content,\n",
    "        name='\"' + component_info.name + '\"',\n",
    "        description='\"' + component_info.description + '\"',\n",
    "        category='\"' + component_info.category + '\"',\n",
    "        id='\"' + str(component_info.id) + '\"',\n",
    "        revision_group_id='\"' + str(component_info.revision_group_id) + '\"',\n",
    "        version_tag='\"' + component_info.version_tag + '\"',\n",
    "        params_list=param_list_str,\n",
    "        main_func_declaration_start=main_func_declaration_start,\n",
    "        opening_bracket = \"{\",\n",
    "        closing_bracket = \"}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_complete_component_module(component_info: ComponentInfo) -> str:\n",
    "    return (\n",
    "        imports_template\n",
    "        + \"\\n\"\n",
    "        + generate_function_header(component_info)\n",
    "        + \"\\n\"\n",
    "        + function_body_template\n",
    "    )\n",
    "\n",
    "\n",
    "def update_code(\n",
    "    existing_code: Optional[str],\n",
    "    component_info: ComponentInfo,\n",
    ") -> str:\n",
    "    \"\"\"Generate and update component code\n",
    "\n",
    "    Tries to replace the existing_code with a new version with the correct function definition\n",
    "    from input_type_dict and output_type_dict.\n",
    "    If no existing_code is provided it completely generates a component module code stub\n",
    "    including necessary imports.\n",
    "\n",
    "    The updating process is rather naive: It does not rely on parsing the abstract syntax tree.\n",
    "    It only uses basic String methods and does not try to handle every case. It therefore may\n",
    "    undesirably replace user code in some cases.\n",
    "    \"\"\"\n",
    "    if existing_code is None or existing_code == \"\":\n",
    "        return generate_complete_component_module(component_info)\n",
    "\n",
    "    new_function_header = generate_function_header(component_info)\n",
    "\n",
    "    try:\n",
    "        start, remaining = existing_code.split(\n",
    "            \"# ***** DO NOT EDIT LINES BELOW *****\", 1\n",
    "        )\n",
    "    except ValueError:\n",
    "        # Cannot find func def, therefore append it (assuming necessary imports are present):\n",
    "        # This may secretely add a second main entrypoint function!\n",
    "        return (\n",
    "            existing_code + \"\\n\\n\" + new_function_header + \"\\n\" + function_body_template\n",
    "        )\n",
    "\n",
    "    if \"    # ***** DO NOT EDIT LINES ABOVE *****\" not in remaining:\n",
    "        # Cannot find end of function definition.\n",
    "        # Therefore replace all code starting from the detected beginning of the function\n",
    "        # definition. This deletes all user code below!\n",
    "        return start + new_function_header + \"\\n\" + function_body_template\n",
    "\n",
    "    # we now are quite sure that we find a complete existing function definition\n",
    "\n",
    "    # pylint: disable=unused-variable\n",
    "    old_func_def, end = remaining.split(\"    # ***** DO NOT EDIT LINES ABOVE *****\", 1)\n",
    "\n",
    "    old_func_def_lines = old_func_def.split(\"\\n\")\n",
    "    use_async_def = (len(old_func_def_lines) >= 3) and old_func_def_lines[\n",
    "        -3\n",
    "    ].startswith(\"async def\")\n",
    "    component_info.is_coroutine = use_async_def\n",
    "\n",
    "    new_function_header = generate_function_header(component_info)\n",
    "\n",
    "    return start + new_function_header + end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_code_files(source_path: str, temp_dir: str) -> None:\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    for root, _, files in os.walk(source_path):\n",
    "        for file in files:\n",
    "            current_path = os.path.join(root, file)\n",
    "            if current_path.endswith(\"json\"):\n",
    "                print(current_path)\n",
    "                with open(current_path, \"r\") as f:\n",
    "                    file_content = f.read()\n",
    "                    transformation_revision = json.loads(file_content)\n",
    "                if transformation_revision[\"type\"] == \"COMPONENT\":\n",
    "                    code_file = os.path.join(temp_dir, file.split(\".\")[0] + \".py\")\n",
    "                    with open(code_file, \"w\", encoding=\"utf8\") as f:\n",
    "                        f.write(transformation_revision[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10987ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_code_files(\n",
    "#     \"./transformations/components/anomaly-detection/\",\n",
    "#     \"./transformations/components/anomaly-detection/\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bb65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_identifier = [\n",
    "    \"alerts-from-score_100_38f168ef-cb06-d89c-79b3-0cd823f32e9d\",\n",
    "    \"isolation-forest_100_cdec1d55-5bb6-8e8d-4571-fbc0ebf5a354\",\n",
    "    \"simple-volatility-score_100_0c3c74d0-89b6-1948-fedd-753eaa47ca0e\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_without_extension = transformation_identifier[2]\n",
    "category_directory = \"anomaly-detection\"\n",
    "py_file_path = os.path.join(\"./transformations/components/\", category_directory, file_without_extension+\".py\")\n",
    "with open(py_file_path, \"r\") as f:\n",
    "    code_from_py_file = f.read()\n",
    "# print(code_from_py_file)\n",
    "json_file_path = os.path.join(\"./transformations/components/\", category_directory, file_without_extension+\".json\")\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    file_content = f.read()\n",
    "tr_json = json.loads(file_content)\n",
    "tr = TransformationRevision(**tr_json)\n",
    "tr.content = update_code(code_from_py_file, ComponentInfo.from_tr(tr))\n",
    "print(tr.content)\n",
    "tr_json = json.dumps(tr.dict(exclude_unset=True), cls=TREncoder, indent=2, sort_keys=True)\n",
    "# with open(json_file_path, \"w\", encoding=\"utf8\") as f:\n",
    "#     json.dump(json.loads(tr_json), f, cls=TREncoder, indent=2, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa46272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
