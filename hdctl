#!/usr/bin/env bash
#
###############################################################################
#                                                                             #
#                                    hdctl                                    #
#                                                                             #
#   Manage maintenance / devops tasks around hetida designer installations    #
#                                                                             #
#                                                                             #
###############################################################################
#
# LICENSE
#   Copyright Â© 2024 fuseki GmbH
#
#   MIT License
#   see https://github.com/hetida/hetida-designer/blob/release/LICENSE
#
# INTRODUCTION
#   hdctl is a bash command line tool for managing typical maintenance /
#   devops tasks around hetida designer installations.
#
#   hdctl can export/import components and workflows from/to the backend api
#   while taking care of authentication. A typical use case is syncing
#   components / workflows from dev instances to test or prod instances.
#
#   Another use case is running maintenance operations, e.g. as described in
#   https://github.com/hetida/hetida-designer/blob/release/docs/cleanup.md
#   from the command line only, i.e. without needing docker.
#
#   hdctl is based on bash/coreutils + curl with the additional requirement
#   of awk and jq for some tasks that involve json wrangling. This enables
#   managing hetida designer instances from locations where no docker or
#   python installation is available or installing arbitrary dependencies
#   and software is difficult due to administrative / security restrictions.
#
# INSTALLATION
#   Just copy this bash script to the environment where you want to run it
#   from. There you need
#     * modern/recent bash (version >4)
#     * GNU coreutils
#     * curl (ideally >= 7.87.0)
#     * optionally awk and jq (https://stedolan.github.io/jq/) for some
#       subcommands that involve json unpacking
#
#   Notes:
#     * hdctl should run under Git Bash on Windows. Recent versions of Git
#       for Windows include curl >= 7.87.0 and gawk. jq must be installed
#       separately. If you experience problems please open an issue.
#     * Using nix, you can start an environment with all dependencies by
#       running
#         nix-shell \
#           -I nixpkgs=https://github.com/NixOS/nixpkgs/archive/nixos-unstable.tar.gz \
#           -p bashInteractive jq gawk coreutils curl
#       At least with the following fixed nixpkgs commit it should
#       definitely work:
#         nix-shell \
#           -I nixpkgs=https://github.com/NixOS/nixpkgs/archive/28319deb5ab05458d9cd5c7d99e1a24ec2e8fc4b.tar.gz \
#           -p bashInteractive jq gawk coreutils curl
#
#
# USAGE
#   Run
#       hdctl usage
#   to see detailed usage instructions and descriptions of all subcommands
#   and arguments.

set -e

######################## Initialize mutable Global variables ##################
ACCESS_TOKEN=""
EXPIRATION_EPOCH=0
_JQ_AVAILABLE=true

###############################################################################
#                                                                             #
#                              Logging functions                              #
#                                                                             #
###############################################################################

_info() {
    if [[ $SILENT = false ]]; then
        echo "${@}" >&2
    fi
}

_debug() {
    if [[ $VERBOSITY == "debug" ]]; then
        echo "${@}" >&2
    fi
}

_log_error() {
    echo "${@}" >&2
}

###############################################################################
#                                                                             #
#               System environment detection helper functions                 #
#                                                                             #
###############################################################################

_detect_jq() {
    if ! command -v jq 1>/dev/null; then
        _info "WARNING: jq not available. Some parameters/options cannot be used."
        _JQ_AVAILABLE=false
    else
        _info "Successfully detected jq in PATH."
    fi
}

JQ_ENSURED=false
_ensure_jq() {
    if ! command -v jq 1>/dev/null; then
        _log_error "Called command needs jq command line tool, which was not found. Aborting."
        exit 127
    fi
    JQ_ENSURED=true
}

_ensure_jq_once() {
    if ! "$JQ_ENSURED"; then
        _ensure_jq
    fi
}

# outputs a certain value from env file
value_from_env_file() {
    local var_name="$1"
    local env_file_path="$2"

    local env_file_entry
    env_file_entry="$(grep -m 1 '^'"$var_name"= "$env_file_path")"

    local str_after_equal="${env_file_entry#*=}" # may contain " or so
    local actual_str
    eval actual_str="$str_after_equal" # to get rid of enclosing ""
    printf '%s' "$actual_str"
}

# shellcheck disable=SC2046
PYTEST_INI_CONTENT=$(
    cat <<'END_HEREDOC'
[pytest]
python_files = *.py
addopts = --doctest-modules
END_HEREDOC
)

# shellcheck disable=SC2046
HDUTILS_PY_CONTENT=$(
    cat <<'END_HEREDOC'
import datetime
import io
import json
import logging
from collections.abc import Generator
from enum import StrEnum
from types import UnionType
from typing import Any, Literal, TypedDict
from uuid import UUID

import numpy as np
import pandas as pd
import pytz
from plotly.graph_objects import Figure
from plotly.utils import PlotlyJSONEncoder
from pydantic import BaseConfig, BaseModel, Field, ValidationError, create_model

logger = logging.getLogger(__name__)

MULTITSFRAME_COLUMN_NAMES = ["timestamp", "metric", "value"]


class ComponentException(Exception):
    """Exception to re-raise exceptions with error code raised in the component code."""

    __is_hetida_designer_exception__ = True

    def __init__(
        self,
        *args: Any,
        error_code: int | str = "",
        extra_information: dict | None = None,
        **kwargs: Any,
    ) -> None:
        if not isinstance(error_code, int | str):
            raise ValueError("The ComponentException.error_code must be int or string!")
        self.error_code = error_code
        self.extra_information = extra_information
        super().__init__(*args, **kwargs)


class ComponentInputValidationException(ComponentException):
    """In code input validation failures"""

    def __init__(
        self,
        *args: Any,
        invalid_component_inputs: list[str],
        error_code: int | str = "",
        **kwargs: Any,
    ) -> None:
        super().__init__(
            *args,
            error_code=error_code,
            extra_information={"invalid_component_inputs": invalid_component_inputs},
            **kwargs,
        )


class WrappedModelWithCustomObjects(BaseModel):
    model: Any
    custom_objects: dict[str, Any]


class MetaDataWrapped(BaseModel):
    """Allows to wrap pandas object data with metadata"""

    hd_wrapped_data_object__: Literal["SERIES", "DATAFRAME"] = Field(
        ..., alias="__hd_wrapped_data_object__"
    )
    metadata__: dict[str, Any] = Field(
        ...,
        alias="__metadata__",
        description="Json serializable dictionary of metadata. Will be written"
        "to the resulting pandas object's attrs attribute.",
    )
    data__: dict | list = Field(
        ...,
        alias="__data__",
        description="The actual data which constitutes the pandas object.",
    )
    data_parsing_options__: dict = Field(
        {},
        alias="__data_parsing_options__",
        description=(
            "Additional options for parsing the provided data."
            " For example, setting orient to one of the allowed values for the respective"
            " Pandas type allows to use different json representations for the actual data."
        ),
    )


def try_parse_wrapped(
    data: str | dict | list,
    hd_wrapped_data_object: Literal["SERIES", "DATAFRAME"],
) -> MetaDataWrapped:
    if isinstance(data, str):
        wrapped_data = MetaDataWrapped.parse_raw(data)  # model_validate_json in pydantic 2.0

        if wrapped_data.hd_wrapped_data_object__ != hd_wrapped_data_object:
            msg = (
                f"Unexpected hd model type: {wrapped_data.hd_wrapped_data_object__}."
                f" Expected {hd_wrapped_data_object}"
            )
            logger.warning(msg)
            raise TypeError(msg)
    else:
        wrapped_data = MetaDataWrapped.parse_obj(data)  # model_validate in pydantic 2.0

    return wrapped_data


def parse_wrapped_content(
    v: str | dict | list,
    wrapped_data_objec: Literal["SERIES", "DATAFRAME"],
) -> tuple[str | dict | list, None | dict[str, Any], dict[str, Any]]:
    data_content: str | dict | list
    try:
        wrapped_object = try_parse_wrapped(v, wrapped_data_objec)
        parsed_metadata = wrapped_object.metadata__
        data_content = wrapped_object.data__
        parsing_options = wrapped_object.data_parsing_options__
    except (ValidationError, TypeError) as e:
        logger.debug("Data object is not wrapped: %s", str(e))
        data_content = v
        parsed_metadata = None
        parsing_options = {}

    return data_content, parsed_metadata, parsing_options


def wrap_metadata_as_attrs(
    data_object: pd.Series | pd.DataFrame, metadata: None | dict[str, Any]
) -> pd.Series | pd.DataFrame:  # TODO: make generic: input type = output type
    if metadata is not None:
        data_object.attrs = metadata
    else:
        data_object.attrs = {}
    return data_object


def parse_pandas_data_content(
    data_content: str | dict | list, typ: Literal["series", "frame"], parsing_options: dict
) -> pd.DataFrame | pd.Series:
    try:
        if isinstance(data_content, str):
            parsed_pandas_object = pd.read_json(
                io.StringIO(data_content), typ=typ, **parsing_options
            )
        else:
            parsed_pandas_object = pd.read_json(data_content, typ=typ, **parsing_options)

    except Exception:  # noqa: BLE001
        try:
            parsed_pandas_object = pd.read_json(
                io.StringIO(json.dumps(data_content)), typ=typ, **parsing_options
            )

        except Exception as read_json_exception:  # noqa: BLE001
            raise ValueError(
                "Could not parse provided input as Pandas "
                + ("Series" if type == "series" else "DataFrame")
            ) from read_json_exception

    return parsed_pandas_object


class DataType(StrEnum):
    """hetida designer data types

    These are the types available for component/workflow inputs/outputs.
    """

    Integer = "INT"
    Float = "FLOAT"
    String = "STRING"
    DataFrame = "DATAFRAME"
    Series = "SERIES"
    MultiTSFrame = "MULTITSFRAME"
    Boolean = "BOOLEAN"
    Any = "ANY"
    PlotlyJson = "PLOTLYJSON"


class PydanticPandasSeries:
    """Custom pydantic Data Type for parsing Pandas Series

    Parses either a json string according to pandas.read_json
    with typ="series" and default arguments otherwise or
    a Python dict-like data structure using the constructor
    of the pandas.Series class with default arguments.

    Also allows a wrapped variant where metadata can be provided.

    Examples of valid input:
        '{"0":1.0,"1":2.1,"2":3.2}'
        {"0":1.0,"1":2.1,"2":3.2}
        [1.2, 3.5, 2.9]
        '[1.2, 3.5, 2.9]'

    """

    @classmethod
    def __get_validators__(cls) -> Generator:
        yield cls.validate

    @classmethod
    def validate(  # noqa: PLR0911,PLR0912
        cls, v: pd.Series | str | dict | list
    ) -> pd.Series:
        if isinstance(v, pd.Series):
            return v

        if not isinstance(
            v, str | dict | list
        ):  # need to check at runtime since we get objects from user code
            msg = f"Got unexpected type at runtime when parsing Series: {str(type(v))}"
            logger.error(msg)
            raise ValueError(msg)

        data_content, metadata, parsing_options = parse_wrapped_content(v, "SERIES")

        return wrap_metadata_as_attrs(
            parse_pandas_data_content(data_content, "series", parsing_options), metadata
        )


class PydanticPandasDataFrame:
    """Custom pydantic Data Type for parsing Pandas DataFrames

    Parses either a json string according to pandas.read_json
    with typ="frame" and default arguments otherwise or
    a Python dict-like data structure using the constructor
    of the pandas.DataFrame class with default arguments.

    Additionally a MetaDataWrapped variant of these can be parsed
    and then is equipped with the provided metadata in the `attrs`
    attribute (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.attrs.html
    """

    @classmethod
    def __get_validators__(cls) -> Generator:
        yield cls.validate

    @classmethod
    def validate(cls, v: pd.DataFrame | str | dict | list) -> pd.DataFrame:
        if isinstance(v, pd.DataFrame):
            return v

        if not isinstance(
            v, str | dict | list
        ):  # need to check at runtime since we get objects from user code
            msg = f"Got unexpected type at runtime when parsing DataFrame: {str(type(v))}"
            logger.error(msg)
            raise ValueError(msg)

        data_content, metadata, parsing_options = parse_wrapped_content(v, "DATAFRAME")

        return wrap_metadata_as_attrs(
            parse_pandas_data_content(data_content, "frame", parsing_options), metadata
        )


class PydanticMultiTimeseriesPandasDataFrame:
    """Custom pydantic Data Type for parsing Multi Timeseries Pandas DataFrames

    Parses data as a dataframe similarly to PydanticPandasDataFrame but
    additionally checks the column layout and types to match the conventions
    for a multitsframe.
    """

    @classmethod
    def __get_validators__(cls) -> Generator:
        yield cls.validate_df
        yield cls.validate_multits_properties

    @classmethod
    def validate_df(cls, v: pd.DataFrame | str | dict | list) -> pd.DataFrame:
        if isinstance(v, pd.DataFrame):
            return v

        if not isinstance(
            v, str | dict | list
        ):  # need to check at runtime since we get objects from user code
            msg = f"Got unexpected type at runtime when parsing MultiTsFrame: {str(type(v))}"
            logger.error(msg)
            raise ValueError(msg)

        data_content, metadata, parsing_options = parse_wrapped_content(v, "DATAFRAME")

        return wrap_metadata_as_attrs(
            parse_pandas_data_content(data_content, "frame", parsing_options), metadata
        )

    @classmethod
    def validate_multits_properties(  # noqa:PLR0912
        cls, df: pd.DataFrame
    ) -> pd.DataFrame:
        if len(df.columns) == 0:
            df = pd.DataFrame(columns=MULTITSFRAME_COLUMN_NAMES)

        if len(df.columns) < 3:
            raise ValueError(
                "MultiTSFrame requires at least 3 columns: metric, timestamp"
                f" and at least one additional columns. Only found {str(df.columns)}"
            )

        if not ({"metric", "timestamp"}.issubset(set(df.columns))):
            column_names_string = ", ".join(df.columns)
            raise ValueError(
                f"The column names {column_names_string} don't contain required columns"
                ' "timestamp" and "metric" for a MultiTSFrame.'
            )

        if df["metric"].isna().any():
            raise ValueError(
                "No null values are allowed for the column 'metric' of a MulitTSFrame."
            )

        df["metric"] = df["metric"].astype("string")

        if df["timestamp"].isna().any():
            raise ValueError(
                "No null values are allowed for the column 'timestamp' of a MulitTSFrame."
            )

        if len(df.index) == 0:
            df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True)

        if not isinstance(df["timestamp"].dtype, pd.DatetimeTZDtype):
            raise ValueError(
                "Column 'timestamp' of MultiTSFrame does not have DatetimeTZDtype dtype. "
                f'Got {str(df["timestamp"].dtype)} index dtype instead.'
            )

        if not df["timestamp"].dt.tz in (pytz.UTC, datetime.timezone.utc):
            raise ValueError(
                "Column 'timestamp' of MultiTSFrame does not have UTC timezone. "
                f'Got {str(df["timestamp"].dt.tz)} timezone instead.'
            )

        return df.sort_values("timestamp")


class ParsedAny:
    """Tries to parse Any objects somehow intelligently

    Reason is that an object may be provided by the backend either as a proper json-object directly
    in some cases (dict-like objects) or as json-encoded string (happens for example for lists).

    Sometimes, if the frontend is involved, json strings get even double-string-encoded! This is a
    known bug of frontend-backend-runtime interaction.

    Sometimes adapter implementations deliver ANY-data directly as json objects and othertimes
    string-encoded.

    As a workaround for all these cases this class tries to json-parse a string if it receives one
    and only if that does not work it yields the actual string value. If it works and the result is
    itself a string again it tries to json-decode a second time and returns the result if that
    works. Otherwise it returns the result string of the first parsing.

    This workaround is justified by the argument that the user should really use a STRING input if
    a string is expected and not an ANY input. Likewise, adapters should offer string data as STRING
    data sources and not as ANY data sources.
    """

    @classmethod
    def __get_validators__(cls) -> Generator:
        yield cls.validate

    @classmethod
    def validate(cls, v: Any) -> Any:
        if isinstance(v, str):
            # try to parse string as json
            try:
                parsed_json_object = json.loads(v)
            except json.decoder.JSONDecodeError:
                logger.info(
                    "Could not JSON-parse string %s in Any input."
                    " Therefore treating it as actual string value",
                    v[:30] + "..." if len(v) > 10 else v,
                )
                return v

            if isinstance(
                parsed_json_object, str
            ):  # sometimes it even gets double-encoded for some reasons
                try:
                    parsed_json_object = json.loads(parsed_json_object)
                except json.decoder.JSONDecodeError:
                    logger.info(
                        "Could not JSON-parse string %s in Any input. "
                        " Therefore treating it as actual string value",
                        parsed_json_object[:30] + "..." if len(v) > 10 else v,
                    )
                    return parsed_json_object

            return parsed_json_object
        return v


data_type_map: dict[DataType, type] = {
    DataType.Integer: int,
    DataType.Float: float,
    DataType.String: str,
    DataType.Series: PydanticPandasSeries,
    DataType.MultiTSFrame: PydanticMultiTimeseriesPandasDataFrame,
    DataType.DataFrame: PydanticPandasDataFrame,
    DataType.Boolean: bool,
    # Any as Type is the correct way to tell pydantic how to parse an arbitrary object:
    DataType.Any: ParsedAny,
    DataType.PlotlyJson: dict,
}

optional_data_type_map: dict[DataType, UnionType] = {
    DataType.Integer: int | None,
    DataType.Float: float | None,
    DataType.String: str | None,
    DataType.Series: PydanticPandasSeries | None,
    DataType.MultiTSFrame: PydanticMultiTimeseriesPandasDataFrame | None,
    DataType.DataFrame: PydanticPandasDataFrame | None,
    DataType.Boolean: bool | None,
    # Any as Type is the correct way to tell pydantic how to parse an arbitrary object:
    DataType.Any: ParsedAny | None,
    DataType.PlotlyJson: dict | None,
}


class AdvancedTypesOutputSerializationConfig(BaseConfig):
    """Enable Pydantic Models to serialize Pandas obejcts with this config class"""

    # Unfortunately there is no good way to get None for NaN or NaT:
    json_encoders = {
        pd.Series: lambda v: {
            "__hd_wrapped_data_object__": "SERIES",
            "__metadata__": v.attrs,
            "__data__": json.loads(
                # double serialization7deserialization in order to serialize both NaN and NaT
                # to null
                v.to_json(
                    date_format="iso",
                    orient="split",
                    # orient="split" serialization is the only way pandas keeps duplicate index
                    #  (with possibly different values) entries for Series objects!
                )
            ),
            "__data_parsing_options__": {"orient": "split"},
        },
        pd.DataFrame: lambda v: {
            "__hd_wrapped_data_object__": "DATAFRAME",
            "__metadata__": v.attrs,
            "__data__": json.loads(
                v.to_json(date_format="iso")  # in order to serialize both NaN and NaT to null
            ),
        },
        PydanticPandasSeries: lambda v: {
            "__hd_wrapped_data_object__": "SERIES",
            "__metadata__": v.attrs,
            "__data__": v.to_dict(),
        },
        PydanticPandasDataFrame: lambda v: {
            "__hd_wrapped_data_object__": "DATAFRAME",
            "__metadata__": v.attrs,
            "__data__": json.loads(
                v.to_json(date_format="iso")  # in order to serialize both NaN and NaT to null
            ),
        },
        PydanticMultiTimeseriesPandasDataFrame: lambda v: {
            "__hd_wrapped_data_object__": "DATAFRAME",
            "__metadata__": v.attrs,
            "__data__": json.loads(
                v.to_json(date_format="iso")
            ),  # in order to serialize both NaN and NaT to null
        },
        np.ndarray: lambda v: v.tolist(),
        datetime.datetime: lambda v: v.isoformat(),
        UUID: lambda v: str(v),  # alternatively: v.hex
        Figure: lambda v: json.loads(json.dumps(v.to_plotly_json(), cls=PlotlyJSONEncoder)),
    }


class NamedDataTypedValue(TypedDict):
    name: str
    type: DataType  # noqa: A003
    value: Any


def parse_via_pydantic(
    entries: list[NamedDataTypedValue],
    type_map: dict[DataType, type] | dict[DataType, UnionType] | None = None,
) -> BaseModel:
    """Parse data dynamically into a pydantic object

    Optionally a type_map can be specified which differs from the default data_type_map

    Returns an instantiated pydantic object if no parsing exception is thrown.

    May raise the typical exceptions of pydantic parsing.
    """
    type_dict: dict[str, tuple[type | UnionType, "ellipsis"]] = {  # noqa: F821
        entry["name"]: (
            type_map[entry["type"]]
            if type_map is not None
            else data_type_map[entry["type"]],  # default to data_type_map
            ...,
        )
        for entry in entries
    }

    DynamicModel = create_model("DynamicyModel", **type_dict)  # type: ignore

    return DynamicModel(**{entry["name"]: entry["value"] for entry in entries})  # type: ignore


def parse_dynamically_from_datatypes(
    entries: list[NamedDataTypedValue], nullable: bool = False
) -> BaseModel:
    return parse_via_pydantic(
        entries, type_map=data_type_map if nullable is False else optional_data_type_map
    )


def parse_single_value_dynamically(
    name: str, value: Any, data_type: DataType, nullable: bool
) -> Any:
    return parse_dynamically_from_datatypes(
        [{"name": name, "type": data_type, "value": value}], nullable
    ).dict()[name]


def parse_value(value: Any, data_type_str: str, nullable: bool) -> Any:
    return parse_single_value_dynamically("some_value", value, DataType(data_type_str), nullable)


def parse_default_value(component_info: dict, input_name: str) -> Any:
    """Parse default value from COMPONENT_INFO dict

    Used in component main function header to parse a default
    value from the COMPONENT_INFO dict for an input.
    """
    inp = component_info["inputs"][input_name]
    return parse_value(inp["default_value"], inp["data_type"], True)


def plotly_fig_to_json_dict(fig: Figure) -> Any:
    """Turn Plotly figure into a Python dict-like object

    This function can be used in visualization components to obtain the
    correct plotly json-like object from a Plotly Figure object.

    See visualization components from the accompanying base components for
    examples on usage.
    """
    # possibly quite inefficient (multiple serialisation / deserialization) but
    # guarantees that the PlotlyJSONEncoder is used and so the resulting Json
    # should be definitely compatible with the plotly javascript library:
    return json.loads(json.dumps(fig.to_plotly_json(), cls=PlotlyJSONEncoder))

END_HEREDOC
)

###############################################################################
#                                                                             #
#                                Usage / help                                 #
#                                                                             #
###############################################################################

usage() {
    if [ -n "${1-}" ]; then
        _log_error "--> ERROR: $1"
    fi

    echo "BASIC USAGE: Run"
    echo "  $0 SUBCOMMAND <parameters>"
    echo "  $0 sync SYNCCOMMAND INSTANCE"
    echo
    echo "EXAMPLES"
    echo
    echo "  SYNC:"
    echo "    Init a new hetida designer instance against which you want to sync"
    echo
    echo "       $0 sync init my-hd-instance"
    echo
    echo "    After following the instructions and setting configuration options"
    echo "    and credentials in the generated files:"
    echo
    echo "       $0 sync pull my-hd-instance    # sync to local dir"
    echo "       $0 sync push my-hd-instance    # push local dir to hd instance"
    echo
    echo "    Sync of the local file content of my-hd-instance to/from another configured"
    echo "    instance named my-other-hd-instance can be done via"
    echo
    echo "       $0 sync push my-other-hd-instance from my-hd-instance"
    echo "       $0 sync pull my-other-hd-instance to my-hd-instance"
    echo
    echo "    It is recommended to use git for versioning of local files and you"
    echo "    should commit before and after each sync operation."
    echo
    echo "  FETCHING:"
    echo "    Write json list of all components from Physics category to stdout"
    echo
    echo "        $0 "'fetch --url-append "?category=Physics&type=COMPONENT"'
    echo
    echo "    Notes: * In these examples configuration like backend api url, auth etc is assumed to"
    echo "             come from ./hdctl.env file or HDCTL_* environment variables."
    echo "           * available query parameters of the backend api are documented in the openapi"
    echo "             documentation of the /api/transformations GET / PUT endpoints and the"
    echo "             /api/maintenance/* endpoints"
    echo "           * --url-append does not urlencode automatically."
    echo
    echo "    With recent curl (>= 7.87.0, from 2023 or later) you can also propagate query params"
    echo "    via --url-query which automatically url encodes them as follows (note the space char"
    echo "    in category):"
    echo
    echo "        $0 "'fetch -- --url-query "category=Anomaly Detection" --url-query "type=COMPONENT"'
    echo
    echo "    Export json list of transformation revisions of category Physics into single file,"
    echo "    pretty printing the json by piping it through jq:"
    echo
    echo "        $0 fetch -- -d "'"category=Physics"'" | jq > physics_trafos.json"
    echo
    echo "    Export all non-deprecated workflows into directory structure at ./exported_trafos including"
    echo "    their dependencies. The directory and subdirectories for type and catgeory will be created"
    echo "    if necessary (This command needs awk and jq!)"
    echo
    echo "        $0 "'fetch --export-directory ./exported_trafos -- \'
    echo '          --url-query "type=WORKFLOW"\'
    echo '          --url-query "include_dependencies=true"\'
    echo '          --url-query "include_deprecated=false"'
    echo
    echo "  PUSHING:"
    echo '    Import from export directory only the workflows with name "Combine two Series'
    echo '    into DataFrame" which are released (there may be drafts or deprecated revisions)'
    echo "    together with their dependencies. Overwrite released trafo revs if necessary:"
    echo
    echo "      $0 "'push --export-trafos ./exported_trafos --\'
    echo '        --url-query "name=Combine two Series into DataFrame"\'
    echo '        --url-query "include_dependencies=true"\'
    echo '        --url-query "allow_overwrite_released=true"\'
    echo '        --url-query "state=RELEASED"'
    echo
    echo "    The response shows which trafo revs actually where imported and which where ignored."
    echo "    You may want to pipe it through jq for pretty printing."
    echo
    echo "    Import released components from json file containing list of trafo revs allowing overwriting"
    echo "    of released components:"
    echo
    echo "      $0 "'push physics_trafos.json --url-append "?type=COMPONENT&state=RELEASED&allow_overwrite_released=true"'
    echo
    echo "  MAINTENANCE:"
    echo
    echo "    Reset test wirings to stored release wirings for released and deprecated trafos:"
    echo
    echo "      $0 "'reset_test_wiring_to_release_wiring -i hddev'
    echo
    echo "    Redeploy base trafos on a remote instance hddev, e.g. after upgrading:"
    echo
    echo "      $0 "'deploy_base_trafos -i hddev --url-query "allow_overwrite_released=true"'
    echo
    echo "    Following examples assume that the maintenance secret is set either in hdctl.env"
    echo "    file (MAINTENANCE_SECRET) or as environment variable HDCTL_MAINTENANCE_SECRET."
    echo
    echo "    Deprecate all but the latest released (i.e. by release date, disregarding version tag!)"
    echo "    revision of every transformation revision group:"
    echo
    echo "      $0 deprecate_all_but_latest_per_group"
    echo
    echo "    Purge your installation: Delete everything and afterwards redeploy base trafos from"
    echo "    running backend (WARNING: irreversible data loss!):"
    echo
    echo "      $0 purge"
    echo
    echo "    Delete all drafts"
    echo
    echo "      $0 delete_drafts"
    echo
    echo "GENERAL HINTS"
    echo "Parameters can be supplied with increasing priority as follows:"
    echo " * On startup a .env file is sourced if available, by default ./hdctl.env."
    echo "   You can configure its path via HDCTL_ENV_FILE environment variable. It should"
    echo "   define variables corresponding to settings of form NAME_IN_UPPER_CASE. In case"
    echo "   of using the sync subcommands instance and credential files are used instead."
    echo " * Environment variables prefixed with HDCTL_ (HDCTL_NAME_IN_UPPER_CASE) are interpreted"
    echo "   and may override NAME_IN_UPPER_CASE from the env file."
    echo " * explicit command line parameters override all these settings"
    echo
    echo "On the command line values to  parameters are separated with space inbetween like"
    echo "    --backend-api-url http://localhost:8080/api"
    echo "or"
    echo "    -b http://localhost:8080/api"
    echo "(i.e. not --backend-api-url=http://localhost:8080/api )"
    echo
    echo "Most parameters are general and can be set independantly of subcommands. Whether they"
    echo "are used depends on the actual subcommand invoked."
    echo
    echo "Arguments after -- are gathered. If the subcommand invokes curl, they are appended to "
    echo "the curl command at the end. This allows for example to usq the --url-query curl argument"
    echo "to control filters when fetching / pushing, as you can see in the examples above."
    echo
    echo "In the following we reference settings by naming them by the variable as it"
    echo "would occur in the .env file (i.e. NAME_IN_UPPER_CASE). Recall that the corresponding"
    echo "environment variable then is HDCTL_NAME_IN_UPPER_CASE."
    echo
    echo "Subcommands typically make http requests to a hd backend api configured via"
    echo "HD_BACKEND_API_URL pointing to the /api resource of the backend service, e.g."
    echo "  http://localhost:8080/api"
    echo
    echo "Authentication is handled as follows:"
    echo "  * if DIRECT_ACCESS_TOKEN is set, it is passed as Bearer token."
    echo "  * if DIRECT_ACCESS_TOKEN is not set / empty string:"
    echo "    * if AUTH_URL is set (e.g. http://localhost/auth)"
    echo "      * if USERNAME is set / non-zero, an access token is tried to be obtained"
    echo "        with PASSWORD from the auth provider at AUTH_URL via openidconnect standard"
    echo "        using password grant credential flow."
    echo "      * if USERNAME is not set, CLIENT_ID and CLIENT_SECRET are used to obtain"
    echo "        an access token from the auth provider at AUTH_URL via openidconnect standard"
    echo "        using client credential grant flow."
    echo "    * if AUTH_URL is not set / empty string the backend is expected to be reachable"
    echo "      without authentication."
    echo
    echo "PARAMETERS"
    echo "  Mandatory arguments to long options are mandatory for short options too. Name of"
    echo "  corresponding environment variable with HDCTL_ prefix in parantheses."
    echo
    echo "  -e | --env-file PATH                     (HDCTL_ENV_FILE)"
    echo "      path to env file that will be sourced at startup. Defaults to ./hdctl.env."
    echo "      If set to non-default value it must exist."
    echo
    echo "  -i | --instance INSTANCE_NAME"
    echo "      Use config from instance files (see sync commands) instead of env file"
    echo
    echo "  -b | --backend-api-url URL              (HDCTL_HD_BACKEND_API_URL)"
    echo "      URL of the backend api resource, e.g. http://localhost:8080/api"
    echo
    echo "  --insecure-backend                      (HDCTL_VERIFY_BACKEND_API_URL)"
    echo "      do not verify https when communicating with the backend."
    echo
    echo "  --add                                   (HDCTL_SYNC_CLEAR_DIRECTORY)"
    echo "      For sync pull commands: If set the target export directory will not be cleared."
    echo "      Trafos will be overwritten / added instead. Note that the environment"
    echo "      variable is true by default and setting --add sets it to false."
    echo
    echo "  -d | --export-directory PATH            (HDCTL_EXPORT_DIRECTORY)"
    echo "      PATH to a directory where data should be exported to (fetch) or loaded from (push)."
    echo "      fetch will create the directory and distribute downloaded trafo revs into json files"
    echo "      at pathes like, using jq and awk"
    echo "          PATH/{workflows,components}/<CATEGORY>/<NAME>_<VERSION_TAG>_<ID>.json"
    echo "      If this is not set, fetch will instead output a json list of all downloaded trafo revs"
    echo "      to stdout."
    echo "      push will use the directory as source if this is set. If not, push expects a path"
    echo "      to a single json file containing an array of trafo revs as first argument instead."
    echo
    echo "  -a | --auth-url AUTH_URL                (HDCTL_AUTH_URL)"
    echo "      Openidconnect auth provider auth endpoint, e.g. https://localhost/auth"
    echo
    echo "  --insecure-auth                         (HDCTL_VERIFY_AUTH_URL)"
    echo "      do not verify https when communicating with auth provider"
    echo
    echo "  -t | --access-token TOKEN               (HDCTL_DIRECT_ACCESS_TOKEN)"
    echo "      If this is provided, it will be used as Bearer access token and no communication"
    echo "      with auth provider will happen. Make sure that TOKEN is not expired."
    echo
    echo "  -r | --realm REALM                      (HDCTL_REALM)"
    echo "      openidconnect realm name that should be used."
    echo
    echo "  -c | --client-id CLIENT_ID              (HDCTL_CLIENT_ID)"
    echo "      The client id. Necessary for both password grant and client credential grant auth flow."
    echo
    echo "  -s | --client-secret CLIENT_SECRET     (HDCTL_CLIENT_SECRET)"
    echo "      The client secret when using client credential secret auth flow"
    echo
    echo "  -m | --maintenance-secret SECRET       (HDCTL_MAINTENANCE_SECRET)"
    echo "      Maintenance actions require a maintenance secret to be configured in the backend"
    echo "      and provided whith each maintenance endpoint request."
    echo
    echo "   -p | --url-append PARAMSTRING         (HDCTL_QUERY_URL_APPEND)"
    echo "       PARAMSTRING will be appended as-is to the url in curl invocations. Can be used"
    echo '       to provide query parameters, for example --url-append "?type=COMPONENT&state=RELEASED"'
    echo
    echo "   -q | --quiet                          (HDCTL_SILENT)"
    echo "       suppress some output"
    echo
    echo "   -v | --verbose                        (HDCTL_VERBOSITY)"
    echo '       The default is "info". Setting it will switch to "debug" and send some additional debug'
    echo "       output to stderr"
    echo
    echo "    --"
    echo "       All command line arguments following -- will be gathered and for curl-invoking"
    echo "       subcommands they will be appended to the curl call."
    echo
    echo "AVAILABLE CUBCOMMANDS BY TOPIC"
    echo "  SYNC PULL / SYNC PUSH / SYNC INIT"
    echo "    Sync is the easiest way for bidirectional transfer of selections of workflows and"
    echo "    components between a local directory and a hetida designer instance. Under the hood"
    echo "    it uses the fetch and push commands described below, but allows configuration and"
    echo "    credentials to reside in local files. In combination with version control, e.g. git,"
    echo "    this enables:"
    echo "      * external version control"
    echo "      * editing locally / hybrid working and frequent switching between local work"
    echo "        and work in the hetida designer user interface"
    echo "      * scripting CD/CI and GitOps"
    echo
    echo "    The configuration and credential files for a remote hetida designer instance"
    echo '    you want to name "my-hd-instance" can be pregenerated via'
    echo
    echo "      $0 sync init my-hd-instance"
    echo
    echo "    Make sure you follow the instructions this command prints out! After that"
    echo "    you should have at least a my-hd-instance.hd-instance file and a "
    echo "    my-hd-instance.hd-creds file which are used as configuration for syncing."
    echo "    Additionally a my-hd-instance.hd-creds.stub file is present that can be checked into"
    echo "    version control together with the .hd-instance file. You should of course never"
    echo "    put the my-hd-instance.hd-creds into version control which you have provided"
    echo "    with actual passwords and secrets. Put *.hd-creds into your .gitignore instead!"
    echo
    echo "    You can now transfer transformations from/to the configured local directory via"
    echo
    echo "      $0 sync pull my-hd-instance"
    echo "      $0 sync push my-hd-instance"
    echo
    echo "    Note that pulling overwrites the local directory completely. It is strongly"
    echo "    recommended to use version control and commit before and after puling and"
    echo "    pushing."
    echo
    echo "    Note that what is pulled and pushed depends on the settings in your .hd-instance"
    echo "    file. The query url parameters of the /api/transformations GET and PUT endpoints"
    echo "    allow for fine-granular filtering and updating / expanding the exported"
    echo "    component code with documentation and a wiring."
    echo
    echo "    Sync of the local file content of my-hd-instance to/from another configured"
    echo "    instance named my-other-hd-instance can be done via"
    echo
    echo "       $0 sync push my-other-hd-instance from my-hd-instance"
    echo "       $0 sync pull my-other-hd-instance to my-hd-instance"
    echo
    echo "    This pushes / pulls with the settings of my-other-hd-instance but uses the"
    echo "    export directory configured for my-hd-instance as source / target."
    echo
    echo "    Finally you can explicitely override the directory to use, e.g.:"
    echo
    echo "       $0 sync push my-prod-instance -d /mounted/volume/dir"
    echo
    echo "  FETCH / EXPORT / DOWNLOAD"
    echo "    The fetch subcommand downloads / exports transformation revisions from the hetida"
    echo "    designer backends using curl, handling auth if necessary. If an export directory"
    echo "    is set it will use jq and awk to distribute the transformation revisions as single"
    echo "    json file each into a directory structure, separating by type and category"
    echo
    echo "      $0 fetch -d EXPORT_DIRECTORY"
    echo
    echo "    Otherwise it will output a json array to stdout"
    echo
    echo "      $0 fetch"
    echo
    echo "    Piping its output to a file creates a single json file containing all fetched"
    echo "    transformation revisions. See -d parameter for details."
    echo
    echo "    By supplying url query parameters as described in the examples at the beginning, only"
    echo "    a part of all available trafo revs can be fetched. See the openapi documentation"
    echo "    of the backend /api/transformations GET endpoint for available parameters."
    echo
    echo "  PUSH / IMPORT / UPLOAD"
    echo "    The push subcommand uploads / imports transformation revisions (in)to the hetida"
    echo "    designer backend using curl, handling auth if necessary. If an export directory"
    echo "    is set it will gather the json files there for the upload:"
    echo
    echo "      $0 push -d -d EXPORT_DIRECTORY"
    echo
    echo "    Otherwise it expects a path to a json file containing a json array of transformation"
    echo "    revisions as first argument:"
    echo
    echo "      $0 push ./exported_trafos.json"
    echo
    echo "    Similarly to the fetch subcommand, by supplying url query parameters, only"
    echo "    a part of all provided trafo revs will be imported. See the openapi documentation"
    echo "    of the backend /api/transformations PUT endpoint for available parameters."
    echo
    echo "  MAINTENANCE"
    echo "    These subcommands correspond to the hd backend api maintenance endpoints."
    echo "    All maintenance subcommands require a non-zero length alphanumeric maintenance"
    echo "    secret to be set at both server and client side:"
    echo "      * server side: configuration of hd backend service (HD_MAINTENANCE_SECRET)"
    echo "      * client side: via -m / --maintenance-secret CLI parameters or environment"
    echo "        variable (HDCTL_MAINTENANCE_SECRET or MAINTENANCE_SECRET in env file)"
    echo
    echo "    See https://github.com/hetida/hetida-designer/blob/release/docs/cleanup.md"
    echo "    or the OpenAPI docs of a started hd backend service for details on what"
    echo "    they do exactly."
    echo
    echo "    Again, query parameters can be provided as described above for fetch and push."
    echo
    echo "    Maintenance subcommands:"
    echo
    echo "      $0 reset_test_wiring_to_release_wiring"
    echo "      $0 deprecate_all_but_latest_per_group"
    echo
    echo ""
    echo "      $0 delete_drafts"
    echo "      $0 delete_unused_deprecated"
    echo "      $0 purge"
    echo "      $0 deploy_base_trafos"
    echo "      $0 autoimport"
    echo
}

help() {
    usage
}

###############################################################################
#                                                                             #
#                           General helper functions                          #
#                                                                             #
###############################################################################

reverse_string() {
    # pipe a string into this function to reverse it!
    var="$(cat)" # read from stdin
    len="${#var}"
    i=0
    rev=""
    while ((i < len)); do rev="${var:i++:1}$rev"; done
    echo "$rev"
}

###############################################################################
#                                                                             #
#                               Token management                              #
#                                                                             #
###############################################################################

get_token_response() {
    curl_args=()
    if [[ $VERIFY_AUTH_URL = false ]]; then
        curl_args+=("--insecure")
    fi

    curl_args+=("-d" "client_id=$CLIENT_ID")

    if [[ -n "$USERNAME" ]]; then
        _debug "Using password credential grant mode."
        curl_args+=("-d" 'grant_type=password' "-d" "username=$USERNAME" "-d" "password=$PASSWORD")
    else
        _debug "Using client credential grant mode"
        curl_args+=("-d" 'grant_type=client_credentials' "-d" "client_secret=$CLIENT_SECRET")
    fi

    curl -s "${curl_args[@]}" "$AUTH_URL/realms/$REALM/protocol/openid-connect/token"
}

extract_access_token() {
    no_space_and_newline="$(echo "$1" | tr -d ' ' | tr -d '\n')"
    _debug "$no_space_and_newline"
    removed_prefix="${no_space_and_newline#*\"access_token\":\"}"
    ACCESS_TOKEN="$(echo "$removed_prefix" | cut -d '"' -f 1)"
}

extract_expiration() {
    no_space_and_newline="$(echo "$1" | tr -d ' ' | tr -d '\n')"
    _debug "$no_space_and_newline"
    removed_prefix="${no_space_and_newline#*\"expires_in\":}"
    echo "$removed_prefix" | cut -d ',' -f 1 | cut -d '}' -f 1
}

set_expiration_epoch() {
    local token_resp
    token_resp="$1"
    local expires_in
    expires_in="$(extract_expiration "$token_resp")"
    issue_epoch=$(date +%s)
    EXPIRATION_EPOCH=$(($issue_epoch + $expires_in - 10)) # 10 seconds buffer
}

get_access_token() {
    local token_resp
    token_resp="$(get_token_response)"
    set_expiration_epoch "$token_resp"
    extract_access_token "$token_resp"
}

check_expiration() {
    if (($(date +%s) > $EXPIRATION_EPOCH)); then
        return 1
    fi
    return 0
}

ensure_access_token() {
    if [[ -n "$DIRECT_ACCESS_TOKEN" ]]; then
        _debug "Using directly provided access token. Expiration unknown!"
        ACCESS_TOKEN="$DIRECT_ACCESS_TOKEN"
        return
    elif [[ -z "$AUTH_URL" ]]; then
        _debug "No auth configured (empty auth url). Not setting token."
        return
    elif [[ -z "$ACCESS_TOKEN" ]] || ! check_expiration; then
        _info "Need to get/renew access token."
        get_access_token
    else
        _debug "Access still token okay."
    fi
}

###############################################################################
#                                                                             #
#                                Maintenance                                  #
#                                                                             #
###############################################################################

##################### ~=~ maintenance helper functions ~=~ ####################

_ensure_maintenance_secret() {
    if [[ -z "$MAINTENANCE_SECRET" ]]; then
        _log_error "Setting maintenance secret required for requested subcommand! Aborting."
        exit 1
    fi
}

######################### ~=~ maintenance actions ~=~ #########################

# Reset test wirings to release wirings

#    Resets the current test wiring to a stored release wiring if present.
#    Only affects released and deprecated transformation revisions.
reset_test_wirings_to_release_wirings() {
    _setup "${@}"
    _ensure_maintenance_secret
    ensure_access_token

    curl_args=()
    if [[ $VERIFY_BACKEND_API_URL = false ]]; then
        curl_args+=("--insecure")
    fi
    curl "${curl_args[@]}" -X POST \
        -H "Authorization: Bearer $ACCESS_TOKEN" \
        -H "Content-Type: application/json" \
        -d '{"maintenance_secret":"'"$MAINTENANCE_SECRET"'"}' \
        "$HD_BACKEND_API_URL/maintenance/reset_test_wiring_to_release_wiring$QUERY_URL_APPEND" \
        "${REMAINING_ARGS[@]}"
}

# Deprecate old transformation revision and keep only latest

#     A transformation revision is considered "old" if it is released and there is a newer
#     released transformation revision (i.e. newer release timestamp) in the same group.

#     Note that this has nothing to to with version tags or any versioning scheme!
deprecate_all_but_latest_per_group() {
    _setup "${@}"
    _ensure_maintenance_secret
    ensure_access_token

    curl_args=()
    if [[ $VERIFY_BACKEND_API_URL = false ]]; then
        curl_args+=("--insecure")
    fi
    curl "${curl_args[@]}" -X POST \
        -H "Authorization: Bearer $ACCESS_TOKEN" \
        -H "Content-Type: application/json" \
        -d '{"maintenance_secret":"'"$MAINTENANCE_SECRET"'"}' \
        "$HD_BACKEND_API_URL/maintenance/deprecate_all_but_latest_per_group$QUERY_URL_APPEND" \
        "${REMAINING_ARGS[@]}"
}

# Delete all transformation revisions with state DRAFT
delete_drafts() {
    _setup "${@}"
    _ensure_maintenance_secret
    ensure_access_token

    curl_args=()
    if [[ $VERIFY_BACKEND_API_URL = false ]]; then
        curl_args+=("--insecure")
    fi
    curl "${curl_args[@]}" -X POST \
        -H "Authorization: Bearer $ACCESS_TOKEN" \
        -H "Content-Type: application/json" \
        -d '{"maintenance_secret":"'"$MAINTENANCE_SECRET"'"}' \
        "$HD_BACKEND_API_URL/maintenance/delete_drafts$QUERY_URL_APPEND" \
        "${REMAINING_ARGS[@]}"
}

# Delete all unused deprecated transformation revisions

#     "Unused" deprecated transformation revisions are those that are either not used in any workflow
#     or only in workflows that themselves are deprecated (and hence will be deleted as well).

#     This handles nesting, i.e. a deprecated trafo rev will not be deleted if it is used indirectly
#     across multiple nesting levels in a workflow which is not deprecated.
delete_unused_deprecated() {
    _setup "${@}"
    _ensure_maintenance_secret
    ensure_access_token

    curl_args=()
    if [[ $VERIFY_BACKEND_API_URL = false ]]; then
        curl_args+=("--insecure")
    fi
    curl "${curl_args[@]}" -X POST \
        -H "Authorization: Bearer $ACCESS_TOKEN" \
        -H "Content-Type: application/json" \
        -d '{"maintenance_secret":"'"$MAINTENANCE_SECRET"'"}' \
        "$HD_BACKEND_API_URL/maintenance/delete_unused_deprecated$QUERY_URL_APPEND" \
        "${REMAINING_ARGS[@]}"
}

# Purge and reinstall base components/workflows

#     This deletes all transformation revisions and afterwards deploys the base
#     components and workflows present in the running backend instance's image at the
#     default path.
purge() {
    _setup "${@}"
    _ensure_maintenance_secret
    ensure_access_token

    curl_args=()
    if [[ $VERIFY_BACKEND_API_URL = false ]]; then
        curl_args+=("--insecure")
    fi
    curl "${curl_args[@]}" -X POST \
        -H "Authorization: Bearer $ACCESS_TOKEN" \
        -H "Content-Type: application/json" \
        -d '{"maintenance_secret":"'"$MAINTENANCE_SECRET"'"}' \
        "$HD_BACKEND_API_URL/maintenance/purge$QUERY_URL_APPEND" \
        "${REMAINING_ARGS[@]}"
}

# Trigger deployment of base transformations from the running backend
# interesting query parameters are
# * allow_overwrite_released
# * update_component_code
deploy_base_trafos() {
    _setup "${@}"
    _ensure_maintenance_secret
    ensure_access_token

    curl_args=()
    if [[ $VERIFY_BACKEND_API_URL = false ]]; then
        curl_args+=("--insecure")
    fi
    curl "${curl_args[@]}" -X POST \
        -H "Authorization: Bearer $ACCESS_TOKEN" \
        -H "Content-Type: application/json" \
        -d '{"maintenance_secret":"'"$MAINTENANCE_SECRET"'"}' \
        "$HD_BACKEND_API_URL/maintenance/deploy_base_trafos$QUERY_URL_APPEND" \
        "${REMAINING_ARGS[@]}"
}

# Trigger autoimport
#
# note that each import is configured by accompagniying config json file, so
# this has no query params
autoimport() {
    _setup "${@}"
    _ensure_maintenance_secret
    ensure_access_token

    curl_args=()
    if [[ $VERIFY_BACKEND_API_URL = false ]]; then
        curl_args+=("--insecure")
    fi
    curl "${curl_args[@]}" -X POST \
        -H "Authorization: Bearer $ACCESS_TOKEN" \
        -H "Content-Type: application/json" \
        -d '{"maintenance_secret":"'"$MAINTENANCE_SECRET"'"}' \
        "$HD_BACKEND_API_URL/maintenance/autoimport$QUERY_URL_APPEND" \
        "${REMAINING_ARGS[@]}"
}

###############################################################################
#                                                                             #
#                              Import into hd                                 #
#                                                                             #
###############################################################################

_import_multi_trafo_json_file() {
    _file_path="$1"
    shift

    ensure_access_token

    # put to /transformations endpoint
    curl_args=()
    if [[ $VERIFY_BACKEND_API_URL = false ]]; then
        curl_args+=("--insecure")
    fi
    curl "${curl_args[@]}" -X PUT -H "Authorization: Bearer $ACCESS_TOKEN" -H "Content-Type: application/json" -d @"$_file_path" "$HD_BACKEND_API_URL/transformations$QUERY_URL_APPEND" "${@}"
}

# encodes as json string, outputs result with "" around
_json_encode_str() {
    printf "%s" "$1" | jq -R -s '.'
}

_import_from_dir() {
    # load all trafos into one big json
    _all_trafos_json='[  ' # 2 extra spaces, see comma + newline removal below

    # trafos from json files
    # shellcheck disable=SC2044
    for f in $(find "$EXPORT_DIRECTORY" -name '*.json'); do
        _all_trafos_json+="$(cat "$f")"
        _all_trafos_json+=","$'\n' # add comma and newline
    done

    # from py files
    # shellcheck disable=SC2044
    for f in $(find "$EXPORT_DIRECTORY" -name '*.py'); do
        if [[ "$(basename "$f")" != "__init__.py" ]] && [[ "$(basename "$f")" != "hdutils.py" ]]; then
            _debug "handle py file $f"

            # only ensure jq if it is needed, i.e. only if py files need
            # to be handled:
            _ensure_jq_once

            local py_file_content
            py_file_content="$(cat "$f")"
            _all_trafos_json+="$(_json_encode_str "$py_file_content")"
            _all_trafos_json+=","$'\n' # add comma and newline
        fi
    done

    _all_trafos_json="${_all_trafos_json::-2}" # remove last comma and newline
    _all_trafos_json+=']'

    ensure_access_token

    local _temp_file
    _temp_file="$(mktemp)"
    echo "$_all_trafos_json" >"$_temp_file"

    curl_args=()
    if [[ $VERIFY_BACKEND_API_URL = false ]]; then
        curl_args+=("--insecure")
    fi

    curl "${curl_args[@]}" -X PUT -H "Authorization: Bearer $ACCESS_TOKEN" -H "Content-Type: application/json" -d @"$_temp_file" "$HD_BACKEND_API_URL/transformations$QUERY_URL_APPEND" "${@}"

    rm "$_temp_file"
}

push() {
    _setup "${@}"
    if [[ -z "$EXPORT_DIRECTORY" ]]; then
        _import_multi_trafo_json_file "${REMAINING_ARGS[@]}"
    else
        _import_from_dir "${REMAINING_ARGS[@]}"
    fi
}

###############################################################################
#                                                                             #
#                              Export from hd                                 #
#                                                                             #
###############################################################################

# Write a json array of all trafos to stdout
_export_to_json() {
    ensure_access_token
    curl_args=()
    if [[ $VERIFY_BACKEND_API_URL = false ]]; then
        curl_args+=("--insecure")
    fi

    curl "${curl_args[@]}" -X GET -H "Authorization: Bearer $ACCESS_TOKEN" -H "Content-Type: application/json" "$HD_BACKEND_API_URL/transformations$QUERY_URL_APPEND" -G "${REMAINING_ARGS[@]}"
}

### Export into dir
_slugify() {
    awk '{
              gsub(/[Ã¤Ã]/,"a");
              gsub(/[Ã¶Ã]/,"o");
              gsub(/[Ã¼Ã]/,"u");
              gsub(/[^0-9a-zA-Z -]/,"");
              gsub(/^[ \t\r\n]+/, "");
              gsub(/[ \t\r\n]+$/, "");
              gsub(/[ ]+/,"_");
              print tolower($0);
        }'
}

_slugify_arg() {
    printf "%s" "$1" | _slugify
}

_subpath_no_slugify() {
    _id="$1"
    _name="$2"
    _category="$3"
    _type="$4"
    _version_tag="$5"
    _extension="${6:-json}"

    printf "%s" "$_type"s/"$_category"/"$_name"_"$_version_tag"_"$_id"."$_extension"
}

_output_hdutils_py_content() {
    echo "$HDUTILS_PY_CONTENT"
}

_output_pytest_ini_content() {
    echo "$PYTEST_INI_CONTENT"
}

_export_to_dir() {
    _ensure_jq
    ensure_access_token
    curl_args=()
    if [[ $VERIFY_BACKEND_API_URL = false ]]; then
        curl_args+=("--insecure")
    fi

    mkdir -p "$EXPORT_DIRECTORY"
    _output_hdutils_py_content >"$EXPORT_DIRECTORY"/hdutils.py
    _output_pytest_ini_content >"$EXPORT_DIRECTORY"/pytest.ini

    # echo curl "${curl_args[@]}" -X GET -G -H "Authorization: Bearer $ACCESS_TOKEN" -H "Content-Type: application/json" "$HD_BACKEND_API_URL/transformations$QUERY_URL_APPEND" "${REMAINING_ARGS[@]}"
    # write each trafo revision to its respective directory

    file_no=0

    for row in $(curl "${curl_args[@]}" -X GET -G -H "Authorization: Bearer $ACCESS_TOKEN" -H "Content-Type: application/json" "$HD_BACKEND_API_URL/transformations$QUERY_URL_APPEND" "${REMAINING_ARGS[@]}" | jq -r '.[] | @base64'); do
        file_no=$(($file_no + 1))
        start=$(date +%s%N)
        decoded_row="$(echo ${row} | base64 --decode)"

        # shellcheck disable=SC2046,SC2005
        read _id _name _category _type _version_tag < <(echo $(echo "$decoded_row" | jq -r '.id, .name, .category, .type, .version_tag' | _slugify))

        #_id="$(echo "$decoded_row" | jq -r '.id')"
        #_name="$(echo "$decoded_row" | jq -r '.name')"
        # _category="$(echo "$decoded_row" | jq -r '.category')"
        #_type="$(echo "$decoded_row" | jq -r '.type')"
        #_version_tag="$(echo "$decoded_row" | jq -r '.version_tag')"

        local extension
        if $EXPORT_COMPONENTS_AS_PY_FILES && [[ "$_type" == "component" ]]; then
            extension="py"
        else
            extension="json"
        fi
        _file_path="$EXPORT_DIRECTORY"/"$(_subpath_no_slugify "$_id" "$_name" "$_category" "$_type" "$_version_tag" "$extension")"

        echo "Extracting $_file_path"

        _debug "Writing file to $_file_path"
        mkdir -p "$(dirname "$_file_path")"

        # actual extraction into file
        if [[ "$extension" == "json" ]]; then
            echo "$decoded_row" | jq >"$_file_path"
        elif [[ "$extension" == "py" ]]; then
            touch "$EXPORT_DIRECTORY/components/__init__.py"
            touch "$(dirname "$_file_path")"/__init__.py
            echo "$decoded_row" | jq -r '.content' >"$_file_path"
        fi

        end=$(date +%s%N)

        runtime=$(((end - start) / 1000000))
        _debug $runtime
    done
    echo "Extracted $file_no files."

}

fetch() {
    _setup "${@}"
    if [[ -z "$EXPORT_DIRECTORY" ]]; then
        _export_to_json
    else
        _export_to_dir
    fi
}

###############################################################################
#                                                                             #
#                                Sync commands                                #
#                                                                             #
###############################################################################

_init_sync_instance() {
    local sync_instance="$1"
    if [[ -z "$sync_instance" ]]; then
        _log_error "Missing sync instance name. Aborting."
        exit 1
    fi

    local instance_file="${sync_instance}.hd-instance"
    local instance_creds_file="${sync_instance}.hd-creds"
    local instance_creds_file_stub="${sync_instance}.hd-creds.stub"

    if [[ -f "$instance_file" ]]; then
        _log_error "Instance file $instance_file already exists. Aborting."
        exit 1
    fi

    local instance_file_content
    instance_file_content=$(
        cat <<END_HEREDOC
HD_BACKEND_API_URL=
VERIFY_BACKEND_API_URL=true


# Auth settings. Actual credentials should be configured in the
# accompanying .hd-creds file.

AUTH_URL=
VERIFY_AUTH_URL=true
REALM=
CLIENT_ID=


EXPORT_DIRECTORY="./$sync_instance-trafos"
EXPORT_COMPONENTS_AS_PY_FILES=true


# Options controlling sync behaviour.
# for example you may add
# id=... or category=... or category_prefix=...
# url query params to filter which transformations to sync.

PULL_QUERY_URL_APPEND='?include_dependencies=true&include_deprecated=false&update_component_code=true&expand_component_code=true'
PUSH_QUERY_URL_APPEND='?include_dependencies=true&allow_overwrite_released=false'

PUSH_ADDITIONAL_ARGUMENTS=()
PULL_ADDITIONAL_ARGUMENTS=()

END_HEREDOC
    )
    echo "$instance_file_content" >"$instance_file"

    local cred_stub_file_content
    cred_stub_file_content=$(
        cat <<END_HEREDOC
MAINTENANCE_SECRET=
USERNAME=
PASSWORD=
CLIENT_SECRET=

END_HEREDOC
    )
    echo "$cred_stub_file_content" >"$instance_creds_file_stub"

    echo "Created files. Next steps:"
    echo ">> 1.) Edit settings in the instance file $instance_file accordingly."
    echo ">> 2.) Edit the credential stub file $instance_creds_file_stub to only contain entries"
    echo ">>     relevant to the authentification method used by the configured hd instance."
    echo ">>     Do not enter passwords yet!"
    echo ">> 3.) Copy the stub file to $instance_creds_file and actually enter passwords"
    echo ">> 4.) Add the cred file (containing passwords) to your gitignore!"
    echo ">> 5.) Add the instance file and the credential stub file to your git repo."
    echo ""
    echo "After that start syncing using the pull and push sync commands:"
    echo "    hdctl sync pull $sync_instance"
    echo "    hdctl sync push $sync_instance"
}

_load_instance_files() {
    local sync_instance="$1"

    if [[ -z "$sync_instance" ]]; then
        _error "No instance specified. Aborting"
        exit 1
    fi

    local instance_file="${sync_instance}.hd-instance"
    local instance_creds_file="${sync_instance}.hd-creds"

    # shellcheck disable=SC1090
    source "$instance_file"
    # shellcheck disable=SC1090
    source "$instance_creds_file"
}

sync() {
    local sync_command="$1"

    if [[ "$sync_command" != "push" ]] && [[ "$sync_command" != "pull" ]] && [[ "$sync_command" != "init" ]]; then
        _log_error "Only allowed sync actions are pull, push and init. Got $sync_command instead. Aborting"
        exit 1
    fi

    local sync_instance="$2"

    if [[ -z "$sync_instance" ]]; then
        _log_error "no hd instance specified. Aborting."
        exit 1
    fi

    shift
    shift

    if [[ "$sync_command" == "init" ]]; then
        _init_sync_instance "$sync_instance"
        return 0
    fi

    _load_instance_files "$sync_instance"

    if [[ "$sync_command" == "pull" ]] && [[ "$1" == "to" ]] && [[ -n "$2" ]]; then

        # pull from instance to local dir of another instance

        shift
        shift
        local other_instance_file="$2".hd-instance

        # overwrite the export directory global variable with the dir from the other instance
        EXPORT_DIRECTORY="$(value_from_env_file "EXPORT_DIRECTORY" "$other_instance_file")"

    elif [[ "$sync_command" == "push" ]] && [[ "$1" == "from" ]] && [[ -n "$2" ]]; then
        # push to instance from local dir of another instance
        local other_instance_file="$2".hd-instance

        # overwrite the export directory global variable with the dir from the other instance
        EXPORT_DIRECTORY="$(value_from_env_file "EXPORT_DIRECTORY" "$other_instance_file")"

        shift
        shift
    fi

    _initialize_variables

    local additional_arguments=()
    if [[ "$sync_command" == "pull" ]]; then
        if [[ -n "$PULL_QUERY_URL_APPEND" ]]; then
            QUERY_URL_APPEND="$PULL_QUERY_URL_APPEND"
        fi
        if [[ -n "$PULL_ADDITIONAL_ARGUMENTS" ]]; then
            additional_arguments=("${PULL_ADDITIONAL_ARGUMENTS[@]}")
        fi
    fi

    if [[ "$sync_command" == "push" ]]; then
        if [[ -n "$PUSH_QUERY_URL_APPEND" ]]; then
            QUERY_URL_APPEND="$PUSH_QUERY_URL_APPEND"
        fi
        if [[ -n "$PUSH_ADDITIONAL_ARGUMENTS" ]]; then
            additional_arguments=("${PUSH_ADDITIONAL_ARGUMENTS[@]}")
        fi
    fi

    _parse_global_options "${additional_arguments[@]}" "${@}"
    _validate_global_parameters

    # Additionally required validations for syncing

    if [[ -z "$EXPORT_DIRECTORY" ]]; then
        _log_error "No export directory specified. Aborting."
        exit 1
    fi

    # Now we are ready to actually run the requested sync command

    if [[ "$sync_command" == "pull" ]]; then
        # clear up directory first
        if "$SYNC_CLEAR_DIRECTORY"; then
            rm -Rf "$EXPORT_DIRECTORY"
        else
            _info ">>> Adding files mode."
        fi

        _export_to_dir
    elif [[ "$sync_command" == "push" ]]; then
        _import_from_dir
    fi

}

###############################################################################
#                                                                             #
#             Setup/handle params, environment variables, env file            #
#                                                                             #
###############################################################################

REMAINING_ARGS=()

# must be called with all command line arguments
_obtain_env_file_config() {
    ENV_FILE_PATH="${HDCTL_ENV_FILE:-"./hdctl.env"}"

    while [[ "$#" -gt 0 ]]; do case $1 in
        -e | --env-file)
            ENV_FILE_PATH="$2"
            shift
            shift
            break
            ;;
        -i | --instance)
            INSTANCE_NAME="$2"
            shift
            shift
            break
            ;;
        --)
            shift
            break
            ;;
        *)
            # ignore everything else
            shift
            ;;
        esac done
}

_source_env_file() {
    if [[ -z "$ENV_FILE_PATH" ]]; then
        _debug "Env file path explicitely unset!"
    elif [[ ! -e "$ENV_FILE_PATH" ]]; then
        if [[ "$ENV_FILE_PATH" == "./hdctl.env" ]]; then
            _debug "No env file at default path. Continue without sourcing"
        else
            _log_error "Explicit env file path set, but could not find file! Aborting."
            exit 1
        fi
    else
        _debug "Sourcing env file $ENV_FILE_PATH."
        # shellcheck disable=SC1090
        source "$ENV_FILE_PATH"
    fi
}

_initialize_variables() {
    # initialize with increasing priority
    # * from defaults
    # * from env file variables of form VARIABLE_NAME
    # * from prefixed environment variables, i.e. of form HDCTL_VARIABLE_NAME

    SILENT="${HDCTL_SILENT:-"${SILENT:-false}"}"
    VERBOSITY="${HDCTL_VERBOSITY:-"${VERBOSITY:-"info"}"}"

    ##### Content
    SYNC_CLEAR_DIRECTORY="${HDCTL_SYNC_CLEAR_DIRECTORY:-"${SYNC_CLEAR_DIRECOTORY:-true}"}"
    EXPORT_DIRECTORY="${HDCTL_EXPORT_DIRECTORY:-"${EXPORT_DIRECTORY:-""}"}"
    EXPORT_COMPONENTS_AS_PY_FILES="${HDCTL_EXPORT_COMPONENTS_AS_PY_FILES:-"${EXPORT_COMPONENTS_AS_PY_FILES:-"true"}"}"

    ##### HD backend

    HD_BACKEND_API_URL="${HDCTL_HD_BACKEND_API_URL:-"${HD_BACKEND_API_URL:-"http://localhost/api"}"}"
    VERIFY_BACKEND_API_URL="${HDCTL_VERIFY_BACKEND_API_URL:-"${VERIFY_BACKEND_API_URL:-true}"}"
    QUERY_URL_APPEND="${HDCTL_QUERY_URL_APPEND:-"${QUERY_URL_APPEND:-""}"}"

    ##### Authentication
    DIRECT_ACCESS_TOKEN="${HDCTL_DIRECT_ACCESS_TOKEN:-"${DIRECT_ACCES_TOKEN:-""}"}"
    AUTH_URL="${HDCTL_AUTH_URL:-"${AUTH_URL:-""}"}"
    USERNAME="${HDCTL_USERNAME:-"${USERNAME:-""}"}"
    PASSWORD="${HDCTL_PASSWORD:-"${PASSWORD:-""}"}"
    CLIENT_ID="${HDCTL_CLIENT_ID:-"${CLIENT_ID:-""}"}"
    CLIENT_SECRET="${HDCTL_CLIENT_SECRET:-"${CLIENT_SECRET:-""}"}"
    REALM="${HDCTL_REALM:-"${REALM:-"hetida"}"}"
    VERIFY_AUTH_URL="${HDCTL_VERIFY_AUTH_URL:-"${VERIFY_AUTH_URL:-true}"}"

    ##### Maintenance authorization
    MAINTENANCE_SECRET="${HDCTL_MAINTENANCE_SECRET:-"${MAINTENANCE_SECRET:-""}"}"
}

_parse_global_options() {

    REMAINING_ARGS=()
    while [[ "$#" -gt 0 ]]; do case $1 in
        -e | --env-file)
            ENV_FILE_PATH="$2"
            shift
            shift
            ;;
        -i | --instance)
            INSTANCE_NAME="$2"
            shift
            shift
            ;;
        -b | --backend-api-url)
            HD_BACKEND_API_URL="$2"
            shift
            shift
            ;;
        --insecure-backend)
            VERIFY_BACKEND_API_URL=false
            shift
            ;;
        --add)
            SYNC_CLEAR_DIRECTORY=false
            shift
            ;;
        -d | --export-directory)
            EXPORT_DIRECTORY="$2"
            shift
            shift
            ;;
        -a | --auth-url)
            AUTH_URL="$2"
            shift
            shift
            ;;
        --insecure-auth)
            VERIFY_AUTH_URL=false
            shift
            ;;
        -t | --access-token)
            DIRECT_ACCESS_TOKEN="$2"
            shift
            shift
            ;;
        -r | --realm)
            REALM="$2"
            shift
            shift
            ;;
        -c | --client-id)
            CLIENT_ID="$2"
            shift
            shift
            ;;
        -s | --client-secret)
            CLIENT_SECRET="$2"
            shift
            shift
            ;;
        -m | --maintenance-secret)
            MAINTENANCE_SECRET="$2"
            shift
            shift
            ;;
        -p | --url-append)
            QUERY_URL_APPEND="$2"
            shift
            shift
            ;;
        -q | --quiet)
            SILENT=true
            shift
            ;;
        -v | --verbose)
            VERBOSITY="debug"
            shift
            ;;
        --)
            shift
            break
            ;;
        *)
            REMAINING_ARGS+=("$1")
            shift
            ;;
            #Unknown parameter appended to REMAINING_ARGS for access by subcommands
        esac done
    REMAINING_ARGS=("${REMAINING_ARGS[@]}" "${@}") # concat with remaining arguments
}

# must be called with cli arguments
_params_setup() {
    _obtain_env_file_config "${@}"
    if [[ -z "$INSTANCE_NAME" ]]; then
        _source_env_file
    else
        _load_instance_files "$INSTANCE_NAME"
    fi
    _initialize_variables
    _parse_global_options "${@}"
}

_validate_global_parameters() {
    # if [[ -n "$AUTH_URL" ]] && [[ -z "$CLIENT_SECRET" ]]; then
    #     _log_error "Auth Url set but no client secret provided. Please provide a client secret"
    #     _log_error 'via -s SECRET or an empty auth url via -a "" to deactivate client credential'
    #     _log_error "auth on the command line. Aborting."
    #     exit 1
    # fi

    if [[ -n "$MAINTENANCE_SECRET" ]]; then
        if [[ ! "$MAINTENANCE_SECRET" =~ ^[a-zA-Z0-9]+$ ]]; then
            _log_error "Only numbers and alphabet letters allowed for the maintenance secret."
            exit 1
        fi
    fi
}

# _setup must be called with args
# typical first function of any subcommand
_setup() {
    _params_setup "${@}"
    _validate_global_parameters
}

###############################################################################
#                                                                             #
#                             Subcommand Execution                            #
#                                                                             #
###############################################################################

_fn_exists() {
    # Checks whether there exists a variable of type function with the given name.
    LC_ALL=C type "${1:-}" 2>/dev/null | grep -q 'function'
}

COMMAND=${1:-usage}
shift || true
ARGUMENTS=("${@}")

if [[ "$COMMAND" == "--help" || "$COMMAND" == "-h" ]]; then
    help
    exit 0
fi

if _fn_exists "$COMMAND"; then
    "$COMMAND" "${ARGUMENTS[@]}"
else
    usage "No subcommand $COMMAND"
fi
